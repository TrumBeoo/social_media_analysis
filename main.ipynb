{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a807f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o file requirements.txt\n",
    "\"\"\"\n",
    "pymongo==4.6.0\n",
    "pandas==2.1.3\n",
    "numpy==1.24.3\n",
    "matplotlib==3.8.2\n",
    "seaborn==0.13.0\n",
    "plotly==5.18.0\n",
    "dash==2.14.2\n",
    "dash-bootstrap-components==1.5.0\n",
    "beautifulsoup4==4.12.2\n",
    "requests==2.31.0\n",
    "snscrape==0.7.0.20230622\n",
    "praw==7.7.1\n",
    "textblob==0.17.1\n",
    "vaderSentiment==3.3.2\n",
    "nltk==3.8.1\n",
    "wordcloud==1.9.3\n",
    "pyvi==0.1.1\n",
    "underthesea==6.7.0\n",
    "scikit-learn==1.3.2\n",
    "pyspark==3.5.0\n",
    "streamlit==1.29.0\n",
    "\"\"\"\n",
    "\n",
    "# C√†i ƒë·∫∑t trong Jupyter Notebook\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4eaa64d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MongoDB connected successfully!\n"
     ]
    }
   ],
   "source": [
    "# cell 1: Import & Setup\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# K·∫øt n·ªëi MongoDB\n",
    "MONGO_URI = \"mongodb+srv://TrumBeoo:1xr1R8BRdLafRzTg@trumbeoo.c0hnfng.mongodb.net/social_media_analysis?\" \\\n",
    "\"retryWrites=true&w=majority&appName=TrumBeoo\"\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client['social_media_analysis']\n",
    "\n",
    "# T·∫°o collections\n",
    "posts_collection = db['posts']\n",
    "analysis_collection = db['sentiment_analysis']\n",
    "trends_collection = db['trends']\n",
    "\n",
    "# T·∫°o indexes ƒë·ªÉ t·ªëi ∆∞u query\n",
    "posts_collection.create_index([(\"created_at\", -1)])\n",
    "posts_collection.create_index([(\"hashtags\", 1)])\n",
    "posts_collection.create_index([(\"topic\", 1)])\n",
    "\n",
    "print(\"‚úÖ MongoDB connected successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd1507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2: Twitter Data Collection v·ªõi snscrape (kh√¥ng c·∫ßn API key)\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class TwitterCrawler:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def search_tweets(self, query, max_results=100, since_date=None):\n",
    "        \"\"\"\n",
    "        Thu th·∫≠p tweets v·ªÅ ch·ªß ƒë·ªÅ c·ª• th·ªÉ b·∫±ng snscrape\n",
    "        \"\"\"\n",
    "        tweets_data = []\n",
    "        \n",
    "        try:\n",
    "            # T·∫°o query v·ªõi ng√†y n·∫øu c√≥\n",
    "            if since_date:\n",
    "                search_query = f\"{query} since:{since_date}\"\n",
    "            else:\n",
    "                # M·∫∑c ƒë·ªãnh l·∫•y tweets trong 7 ng√†y qua\n",
    "                since_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "                search_query = f\"{query} since:{since_date}\"\n",
    "            \n",
    "            print(f\"Searching: {search_query}\")\n",
    "            \n",
    "            # S·ª≠ d·ª•ng snscrape ƒë·ªÉ thu th·∫≠p tweets\n",
    "            tweets = sntwitter.TwitterSearchScraper(search_query).get_items()\n",
    "            \n",
    "            count = 0\n",
    "            for tweet in tweets:\n",
    "                if count >= max_results:\n",
    "                    break\n",
    "                \n",
    "                # Extract hashtags t·ª´ text\n",
    "                hashtags = re.findall(r'#\\w+', tweet.content)\n",
    "                hashtags = [tag[1:] for tag in hashtags]  # B·ªè d·∫•u #\n",
    "                \n",
    "                tweet_doc = {\n",
    "                    'tweet_id': str(tweet.id),\n",
    "                    'text': tweet.content,\n",
    "                    'created_at': tweet.date,\n",
    "                    'likes': tweet.likeCount or 0,\n",
    "                    'retweets': tweet.retweetCount or 0,\n",
    "                    'replies': tweet.replyCount or 0,\n",
    "                    'hashtags': hashtags,\n",
    "                    'lang': tweet.lang or 'unknown',\n",
    "                    'username': tweet.user.username,\n",
    "                    'user_followers': tweet.user.followersCount or 0,\n",
    "                    'topic': query,\n",
    "                    'source': 'twitter',\n",
    "                    'collected_at': datetime.now()\n",
    "                }\n",
    "                tweets_data.append(tweet_doc)\n",
    "                count += 1\n",
    "            \n",
    "            return tweets_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting tweets: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def save_to_mongodb(self, tweets_data):\n",
    "        \"\"\"L∆∞u v√†o MongoDB\"\"\"\n",
    "        if tweets_data:\n",
    "            # Ki·ªÉm tra tr√πng l·∫∑p tr∆∞·ªõc khi insert\n",
    "            new_tweets = []\n",
    "            for tweet in tweets_data:\n",
    "                existing = posts_collection.find_one({'tweet_id': tweet['tweet_id']})\n",
    "                if not existing:\n",
    "                    new_tweets.append(tweet)\n",
    "            \n",
    "            if new_tweets:\n",
    "                posts_collection.insert_many(new_tweets)\n",
    "                print(f\"‚úÖ Saved {len(new_tweets)} new tweets to MongoDB\")\n",
    "            else:\n",
    "                print(\"‚ÑπÔ∏è No new tweets to save\")\n",
    "\n",
    "# S·ª≠ d·ª•ng snscrape (kh√¥ng c·∫ßn API key)\n",
    "crawler = TwitterCrawler()\n",
    "\n",
    "# Thu th·∫≠p d·ªØ li·ªáu v·ªÅ c√°c ch·ªß ƒë·ªÅ\n",
    "topics = [\n",
    "    \"AI education\",\n",
    "    \"tr√≠ tu·ªá nh√¢n t·∫°o gi√°o d·ª•c\", \n",
    "    \"AI h·ªçc t·∫≠p\",\n",
    "    \"#AIEducation\",\n",
    "    \"machine learning gi√°o d·ª•c\"\n",
    "]\n",
    "\n",
    "for topic in topics:\n",
    "    print(f\"üîç Collecting tweets about: {topic}\")\n",
    "    tweets = crawler.search_tweets(topic, max_results=50)\n",
    "    crawler.save_to_mongodb(tweets)\n",
    "    print(f\"Found {len(tweets)} tweets\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5465ddd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 100 Reddit posts to MongoDB\n",
      "‚úÖ Saved 100 Reddit posts to MongoDB\n",
      "‚úÖ Saved 100 Reddit posts to MongoDB\n"
     ]
    }
   ],
   "source": [
    "# cell 3: Reddit Data Collection\n",
    "import praw\n",
    "from datetime import datetime\n",
    "\n",
    "class RedditCrawler:\n",
    "    def __init__(self, client_id, client_secret, user_agent):\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            user_agent=user_agent\n",
    "        )\n",
    "    \n",
    "    def search_posts(self, query, subreddit_name='all', limit=100):\n",
    "        \"\"\"Thu th·∫≠p posts t·ª´ Reddit\"\"\"\n",
    "        posts_data = []\n",
    "        \n",
    "        try:\n",
    "            subreddit = self.reddit.subreddit(subreddit_name)\n",
    "            \n",
    "            for post in subreddit.search(query, limit=limit, sort='new'):\n",
    "                post_doc = {\n",
    "                    'post_id': post.id,\n",
    "                    'title': post.title,\n",
    "                    'text': post.selftext,\n",
    "                    'created_at': datetime.fromtimestamp(post.created_utc),\n",
    "                    'score': post.score,\n",
    "                    'num_comments': post.num_comments,\n",
    "                    'upvote_ratio': post.upvote_ratio,\n",
    "                    'subreddit': post.subreddit.display_name,\n",
    "                    'author': str(post.author),\n",
    "                    'url': post.url,\n",
    "                    'topic': query,\n",
    "                    'source': 'reddit',\n",
    "                    'collected_at': datetime.now()\n",
    "                }\n",
    "                posts_data.append(post_doc)\n",
    "            \n",
    "            return posts_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error collecting Reddit posts: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def save_to_mongodb(self, posts_data):\n",
    "        if posts_data:\n",
    "            posts_collection.insert_many(posts_data)\n",
    "            print(f\"‚úÖ Saved {len(posts_data)} Reddit posts to MongoDB\")\n",
    "\n",
    "# S·ª≠ d·ª•ng\n",
    "reddit_crawler = RedditCrawler(\n",
    "    client_id=\"k6ozqL3mwwC0cGNUSmcdlQ\",\n",
    "    client_secret=\"JR6XLrrWpp2oNi5RNk0uV2GrrCaelw\",\n",
    "    user_agent=\"windows:ai-trend-collector:v1.0 (by /u/trung_it)\"\n",
    ")\n",
    "\n",
    "reddit_topics = [\"AI education\", \"artificial intelligence learning\", \"EdTech\"]\n",
    "for topic in reddit_topics:\n",
    "    posts = reddit_crawler.search_posts(topic, limit=100)\n",
    "    reddit_crawler.save_to_mongodb(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfa6955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated and saved 1000 mock posts\n",
      "\n",
      "üìä Total posts in database: 1600\n"
     ]
    }
   ],
   "source": [
    "# cell 4: Generate Mock Data\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class MockDataGenerator:\n",
    "    def __init__(self):\n",
    "        self.topics = [\"AI trong gi√°o d·ª•c\", \"tr√≠ tu·ªá nh√¢n t·∫°o\", \"h·ªçc m√°y\", \"EdTech\"]\n",
    "        self.hashtags = [\n",
    "            \"AIEducation\", \"EdTech\", \"MachineLearning\", \"DeepLearning\",\n",
    "            \"Gi√°oD·ª•c\", \"H·ªçcT·∫≠pAI\", \"Tr√≠Tu·ªáNh√¢nT·∫°o\", \"C√¥ngNgh·ªáGi√°oD·ª•c\"\n",
    "        ]\n",
    "        self.positive_words = [\n",
    "            \"tuy·ªát v·ªùi\", \"hi·ªáu qu·∫£\", \"h·ªØu √≠ch\", \"ti·ªán l·ª£i\", \"s√°ng t·∫°o\",\n",
    "            \"amazing\", \"excellent\", \"great\", \"helpful\", \"innovative\"\n",
    "        ]\n",
    "        self.negative_words = [\n",
    "            \"kh√≥\", \"ph·ª©c t·∫°p\", \"t·ªën k√©m\", \"lo ng·∫°i\", \"r·ªßi ro\",\n",
    "            \"difficult\", \"complex\", \"expensive\", \"worried\", \"risk\"\n",
    "        ]\n",
    "        self.neutral_words = [\n",
    "            \"nghi√™n c·ª©u\", \"ph√°t tri·ªÉn\", \"·ª©ng d·ª•ng\", \"th·∫£o lu·∫≠n\",\n",
    "            \"research\", \"development\", \"application\", \"discussion\"\n",
    "        ]\n",
    "    \n",
    "    def generate_post(self, sentiment_type='mixed'):\n",
    "        \"\"\"T·∫°o post gi·∫£ l·∫≠p\"\"\"\n",
    "        if sentiment_type == 'positive':\n",
    "            words = self.positive_words\n",
    "        elif sentiment_type == 'negative':\n",
    "            words = self.negative_words\n",
    "        else:\n",
    "            words = self.positive_words + self.negative_words + self.neutral_words\n",
    "        \n",
    "        text = f\"{random.choice(self.topics)} {random.choice(words)} \" \\\n",
    "               f\"trong {random.choice(['l·ªõp h·ªçc', 'tr∆∞·ªùng h·ªçc', 'ƒë·∫°i h·ªçc', 'kh√≥a h·ªçc'])}. \" \\\n",
    "               f\"#{random.choice(self.hashtags)} #{random.choice(self.hashtags)}\"\n",
    "        \n",
    "        days_ago = random.randint(0, 90)\n",
    "        created_at = datetime.now() - timedelta(days=days_ago)\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'created_at': created_at,\n",
    "            'likes': random.randint(0, 1000),\n",
    "            'retweets': random.randint(0, 500),\n",
    "            'replies': random.randint(0, 200),\n",
    "            'hashtags': random.sample(self.hashtags, random.randint(1, 3)),\n",
    "            'topic': random.choice(self.topics),\n",
    "            'source': 'mock_data',\n",
    "            'collected_at': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def generate_dataset(self, num_posts=1000):\n",
    "        \"\"\"T·∫°o dataset v·ªõi t·ª∑ l·ªá c·∫£m x√∫c c√¢n b·∫±ng\"\"\"\n",
    "        posts = []\n",
    "        \n",
    "        # 40% positive, 30% negative, 30% neutral\n",
    "        for _ in range(int(num_posts * 0.4)):\n",
    "            posts.append(self.generate_post('positive'))\n",
    "        \n",
    "        for _ in range(int(num_posts * 0.3)):\n",
    "            posts.append(self.generate_post('negative'))\n",
    "        \n",
    "        for _ in range(int(num_posts * 0.3)):\n",
    "            posts.append(self.generate_post('mixed'))\n",
    "        \n",
    "        return posts\n",
    "    \n",
    "    def save_to_mongodb(self, posts):\n",
    "        if posts:\n",
    "            posts_collection.insert_many(posts)\n",
    "            print(f\"‚úÖ Generated and saved {len(posts)} mock posts\")\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu m·∫´u\n",
    "generator = MockDataGenerator()\n",
    "mock_posts = generator.generate_dataset(1000)\n",
    "generator.save_to_mongodb(mock_posts)\n",
    "\n",
    "# Ki·ªÉm tra d·ªØ li·ªáu\n",
    "print(f\"\\nüìä Total posts in database: {posts_collection.count_documents({})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16ca6646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 100 posts...\n",
      "Analyzed 200 posts...\n",
      "Analyzed 300 posts...\n",
      "Analyzed 400 posts...\n",
      "Analyzed 500 posts...\n",
      "Analyzed 600 posts...\n",
      "Analyzed 700 posts...\n",
      "Analyzed 800 posts...\n",
      "Analyzed 900 posts...\n",
      "Analyzed 1000 posts...\n",
      "Analyzed 1100 posts...\n",
      "Analyzed 1200 posts...\n",
      "Analyzed 1300 posts...\n",
      "‚úÖ Completed sentiment analysis for 1300 posts\n"
     ]
    }
   ],
   "source": [
    "# cell 5: Vietnamese Sentiment Analysis\n",
    "from underthesea import sentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # T·ª´ ƒëi·ªÉn c·∫£m x√∫c ti·∫øng Vi·ªát t√πy ch·ªânh\n",
    "        self.vietnamese_positive = [\n",
    "            't·ªët', 'hay', 'tuy·ªát', 'hi·ªáu qu·∫£', 'h·ªØu √≠ch', 'ti·ªán l·ª£i', \n",
    "            's√°ng t·∫°o', 'xu·∫•t s·∫Øc', 'ho√†n h·∫£o', 'th√∫ v·ªã'\n",
    "        ]\n",
    "        self.vietnamese_negative = [\n",
    "            'x·∫•u', 'k√©m', 't·ªá', 'kh√≥', 'ph·ª©c t·∫°p', 'lo ng·∫°i', \n",
    "            'r·ªßi ro', 'nguy hi·ªÉm', 'th·∫•t b·∫°i', 'kh√¥ng t·ªët'\n",
    "        ]\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"L√†m s·∫°ch text\"\"\"\n",
    "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "        text = re.sub(r'#', '', text)  # Remove hashtag symbol\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        return text.lower().strip()\n",
    "    \n",
    "    def analyze_vietnamese(self, text):\n",
    "        \"\"\"Ph√¢n t√≠ch c·∫£m x√∫c ti·∫øng Vi·ªát\"\"\"\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # ƒê·∫øm t·ª´ t√≠ch c·ª±c v√† ti√™u c·ª±c\n",
    "        positive_count = sum(1 for word in self.vietnamese_positive if word in cleaned_text)\n",
    "        negative_count = sum(1 for word in self.vietnamese_negative if word in cleaned_text)\n",
    "        \n",
    "        # S·ª≠ d·ª•ng underthesea\n",
    "        try:\n",
    "            underthesea_result = sentiment(text)\n",
    "            if underthesea_result == 'positive':\n",
    "                base_score = 0.6\n",
    "            elif underthesea_result == 'negative':\n",
    "                base_score = -0.6\n",
    "            else:\n",
    "                base_score = 0.0\n",
    "        except:\n",
    "            base_score = 0.0\n",
    "        \n",
    "        # T√≠nh ƒëi·ªÉm cu·ªëi c√πng\n",
    "        score = base_score + (positive_count * 0.1) - (negative_count * 0.1)\n",
    "        score = max(-1, min(1, score))  # Gi·ªõi h·∫°n [-1, 1]\n",
    "        \n",
    "        # Ph√¢n lo·∫°i\n",
    "        if score >= 0.2:\n",
    "            label = 'positive'\n",
    "        elif score <= -0.2:\n",
    "            label = 'negative'\n",
    "        else:\n",
    "            label = 'neutral'\n",
    "        \n",
    "        return {\n",
    "            'score': round(score, 3),\n",
    "            'label': label,\n",
    "            'positive_words': positive_count,\n",
    "            'negative_words': negative_count\n",
    "        }\n",
    "    \n",
    "    def analyze_english(self, text):\n",
    "        \"\"\"Ph√¢n t√≠ch c·∫£m x√∫c ti·∫øng Anh\"\"\"\n",
    "        # VADER Sentiment\n",
    "        vader_scores = self.vader.polarity_scores(text)\n",
    "        compound_score = vader_scores['compound']\n",
    "        \n",
    "        if compound_score >= 0.05:\n",
    "            label = 'positive'\n",
    "        elif compound_score <= -0.05:\n",
    "            label = 'negative'\n",
    "        else:\n",
    "            label = 'neutral'\n",
    "        \n",
    "        return {\n",
    "            'score': round(compound_score, 3),\n",
    "            'label': label,\n",
    "            'vader_scores': vader_scores\n",
    "        }\n",
    "    \n",
    "    def analyze(self, text, lang='auto'):\n",
    "        \"\"\"Ph√¢n t√≠ch t·ª± ƒë·ªông d·ª±a v√†o ng√¥n ng·ªØ\"\"\"\n",
    "        if lang == 'auto':\n",
    "            # Ph√°t hi·ªán ng√¥n ng·ªØ ƒë∆°n gi·∫£n\n",
    "            if any(ord(char) > 127 for char in text):\n",
    "                lang = 'vi'\n",
    "            else:\n",
    "                lang = 'en'\n",
    "        \n",
    "        if lang == 'vi':\n",
    "            return self.analyze_vietnamese(text)\n",
    "        else:\n",
    "            return self.analyze_english(text)\n",
    "\n",
    "# √Åp d·ª•ng ph√¢n t√≠ch c·∫£m x√∫c cho t·∫•t c·∫£ posts\n",
    "analyzer = SentimentAnalyzer()\n",
    "\n",
    "def analyze_all_posts():\n",
    "    \"\"\"Ph√¢n t√≠ch c·∫£m x√∫c cho t·∫•t c·∫£ posts trong database\"\"\"\n",
    "    posts = posts_collection.find({'sentiment': {'$exists': False}})\n",
    "    count = 0\n",
    "    \n",
    "    for post in posts:\n",
    "        sentiment_result = analyzer.analyze(post.get('text', ''))\n",
    "        \n",
    "        # C·∫≠p nh·∫≠t document\n",
    "        posts_collection.update_one(\n",
    "            {'_id': post['_id']},\n",
    "            {'$set': {\n",
    "                'sentiment': sentiment_result['label'],\n",
    "                'sentiment_score': sentiment_result['score'],\n",
    "                'analyzed_at': datetime.now()\n",
    "            }}\n",
    "        )\n",
    "        count += 1\n",
    "        \n",
    "        if count % 100 == 0:\n",
    "            print(f\"Analyzed {count} posts...\")\n",
    "    \n",
    "    print(f\"‚úÖ Completed sentiment analysis for {count} posts\")\n",
    "\n",
    "analyze_all_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4d6b92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Sentiment Analysis by Topic:\n",
      "                                _id  total_posts  positive  negative  neutral  \\\n",
      "0                            EdTech          441        99       171      171   \n",
      "1                 AI trong gi√°o d·ª•c          264        43       170       51   \n",
      "2                           h·ªçc m√°y          253        45       153       55   \n",
      "3                  tr√≠ tu·ªá nh√¢n t·∫°o          242        39       136       67   \n",
      "4                      AI education          200        21        29      150   \n",
      "5  artificial intelligence learning          200        52        23      125   \n",
      "\n",
      "   avg_sentiment_score   avg_likes  positive_pct  negative_pct  neutral_pct  \n",
      "0            -0.073454  497.315353         22.45         38.78        38.78  \n",
      "1            -0.309848  489.193182         16.29         64.39        19.32  \n",
      "2            -0.275494  532.252964         17.79         60.47        21.74  \n",
      "3            -0.255372  498.152893         16.12         56.20        27.69  \n",
      "4            -0.008835         NaN         10.50         14.50        75.00  \n",
      "5             0.156450         NaN         26.00         11.50        62.50  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InsertManyResult(['EdTech', 'AI trong gi√°o d·ª•c', 'h·ªçc m√°y', 'tr√≠ tu·ªá nh√¢n t·∫°o', 'AI education', 'artificial intelligence learning'], acknowledged=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 6: Topic-based Analysis\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_by_topic():\n",
    "    \"\"\"Ph√¢n t√≠ch c·∫£m x√∫c theo t·ª´ng ch·ªß ƒë·ªÅ\"\"\"\n",
    "    pipeline = [\n",
    "        {\n",
    "            '$group': {\n",
    "                '_id': '$topic',\n",
    "                'total_posts': {'$sum': 1},\n",
    "                'positive': {\n",
    "                    '$sum': {'$cond': [{'$eq': ['$sentiment', 'positive']}, 1, 0]}\n",
    "                },\n",
    "                'negative': {\n",
    "                    '$sum': {'$cond': [{'$eq': ['$sentiment', 'negative']}, 1, 0]}\n",
    "                },\n",
    "                'neutral': {\n",
    "                    '$sum': {'$cond': [{'$eq': ['$sentiment', 'neutral']}, 1, 0]}\n",
    "                },\n",
    "                'avg_sentiment_score': {'$avg': '$sentiment_score'},\n",
    "                'avg_likes': {'$avg': '$likes'}\n",
    "            }\n",
    "        },\n",
    "        {'$sort': {'total_posts': -1}}\n",
    "    ]\n",
    "    \n",
    "    results = list(posts_collection.aggregate(pipeline))\n",
    "    \n",
    "    # Chuy·ªÉn sang DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    df['positive_pct'] = (df['positive'] / df['total_posts'] * 100).round(2)\n",
    "    df['negative_pct'] = (df['negative'] / df['total_posts'] * 100).round(2)\n",
    "    df['neutral_pct'] = (df['neutral'] / df['total_posts'] * 100).round(2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "topic_analysis = analyze_by_topic()\n",
    "print(\"\\nüìä Sentiment Analysis by Topic:\")\n",
    "print(topic_analysis)\n",
    "\n",
    "# L∆∞u v√†o MongoDB collection ri√™ng\n",
    "analysis_records = topic_analysis.to_dict('records')\n",
    "analysis_collection.delete_many({})  # Clear old data\n",
    "analysis_collection.insert_many(analysis_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07bcf351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Hashtags:\n",
      "#Tr√≠Tu·ªáNh√¢nT·∫°o: 262 mentions\n",
      "#C√¥ngNgh·ªáGi√°oD·ª•c: 258 mentions\n",
      "#AIEducation: 256 mentions\n",
      "#Gi√°oD·ª•c: 252 mentions\n",
      "#H·ªçcT·∫≠pAI: 249 mentions\n",
      "#EdTech: 248 mentions\n",
      "#DeepLearning: 242 mentions\n",
      "#MachineLearning: 241 mentions\n",
      "\n",
      "Engagement Statistics:\n",
      "avg_likes: 504.21\n",
      "avg_retweets: 251.63\n",
      "avg_replies: 100.44\n",
      "avg_score: 5.30\n",
      "total_posts: 1600\n",
      "sentiment_distribution: {'negative': 682, 'neutral': 619, 'positive': 299}\n",
      "Trends analysis saved to database\n"
     ]
    }
   ],
   "source": [
    "# cell 7: Trend Analysis (CORRECTED VERSION)\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "class TrendAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.df = pd.DataFrame(list(posts_collection.find()))\n",
    "    \n",
    "    def get_top_hashtags(self, limit=10):\n",
    "        \"\"\"L·∫•y top hashtags ph·ªï bi·∫øn\"\"\"\n",
    "        all_hashtags = []\n",
    "        \n",
    "        # Ki·ªÉm tra xem c√≥ c·ªôt hashtags kh√¥ng\n",
    "        if 'hashtags' in self.df.columns:\n",
    "            for hashtags in self.df['hashtags'].dropna():\n",
    "                if isinstance(hashtags, list):\n",
    "                    all_hashtags.extend(hashtags)\n",
    "        else:\n",
    "            # N·∫øu kh√¥ng c√≥ hashtags, extract t·ª´ text\n",
    "            for text in self.df['text'].dropna():\n",
    "                hashtags = re.findall(r'#\\w+', str(text))\n",
    "                hashtags = [tag[1:] for tag in hashtags]  # B·ªè d·∫•u #\n",
    "                all_hashtags.extend(hashtags)\n",
    "        \n",
    "        if not all_hashtags:\n",
    "            return [(\"No hashtags found\", 0)]\n",
    "        \n",
    "        hashtag_counts = Counter(all_hashtags)\n",
    "        return hashtag_counts.most_common(limit)\n",
    "    \n",
    "    def get_sentiment_trend(self, days=30):\n",
    "        \"\"\"Ph√¢n t√≠ch xu h∆∞·ªõng c·∫£m x√∫c theo th·ªùi gian\"\"\"\n",
    "        # L·ªçc d·ªØ li·ªáu trong N ng√†y g·∫ßn ƒë√¢y\n",
    "        recent_date = datetime.now() - timedelta(days=days)\n",
    "        recent_posts = self.df[self.df['created_at'] >= recent_date]\n",
    "        \n",
    "        # Group by date v√† sentiment\n",
    "        daily_sentiment = recent_posts.groupby([\n",
    "            recent_posts['created_at'].dt.date, 'sentiment'\n",
    "        ]).size().unstack(fill_value=0)\n",
    "        \n",
    "        return daily_sentiment\n",
    "    \n",
    "    def get_engagement_stats(self):\n",
    "        \"\"\"Th·ªëng k√™ engagement\"\"\"\n",
    "        stats = {\n",
    "            'avg_likes': self.df['likes'].mean() if 'likes' in self.df.columns else 0,\n",
    "            'avg_retweets': self.df['retweets'].mean() if 'retweets' in self.df.columns else 0,\n",
    "            'avg_replies': self.df['replies'].mean() if 'replies' in self.df.columns else 0,\n",
    "            'avg_score': self.df['score'].mean() if 'score' in self.df.columns else 0,\n",
    "            'total_posts': len(self.df),\n",
    "            'sentiment_distribution': self.df['sentiment'].value_counts().to_dict() if 'sentiment' in self.df.columns else {}\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    def save_trends_to_db(self):\n",
    "        \"\"\"L∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch v√†o database\"\"\"\n",
    "        trends_data = {\n",
    "            'analysis_date': datetime.now(),\n",
    "            'top_hashtags': dict(self.get_top_hashtags()),\n",
    "            'engagement_stats': self.get_engagement_stats(),\n",
    "            'total_posts_analyzed': len(self.df)\n",
    "        }\n",
    "        \n",
    "        trends_collection.delete_many({})  # Clear old data\n",
    "        trends_collection.insert_one(trends_data)\n",
    "        print(\"Trends analysis saved to database\")\n",
    "\n",
    "trend_analyzer = TrendAnalyzer()\n",
    "\n",
    "print(\"\\nTop 10 Hashtags:\")\n",
    "for hashtag, count in trend_analyzer.get_top_hashtags(10):\n",
    "    print(f\"#{hashtag}: {count} mentions\")\n",
    "\n",
    "print(\"\\nEngagement Statistics:\")\n",
    "stats = trend_analyzer.get_engagement_stats()\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "trend_analyzer.save_trends_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03615f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 8: Dashboard Setup\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Initialize Dash app\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "# Load data t·ª´ MongoDB\n",
    "def load_data():\n",
    "    posts = list(posts_collection.find())\n",
    "    df = pd.DataFrame(posts)\n",
    "    \n",
    "    if 'created_at' in df.columns:\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        df['date'] = df['created_at'].dt.date\n",
    "        df['month'] = df['created_at'].dt.to_period('M').astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "trend_data = trends_collection.find_one()\n",
    "\n",
    "# Colors\n",
    "COLORS = {\n",
    "    'positive': '#2ecc71',\n",
    "    'negative': '#e74c3c',\n",
    "    'neutral': '#95a5a6',\n",
    "    'background': '#f8f9fa',\n",
    "    'card': '#ffffff'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f2156eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 9: Dashboard Layout\n",
    "app.layout = dbc.Container([\n",
    "    # Header\n",
    "    dbc.Row([\n",
    "        dbc.Col([\n",
    "            html.H1(\"üìä Dashboard Ph√¢n t√≠ch D·ªØ li·ªáu X√£ h·ªôi\", \n",
    "                   className=\"text-center mb-4 mt-4\",\n",
    "                   style={'color': '#2c3e50', 'fontWeight': 'bold'}),\n",
    "            html.H5(\"Ch·ªß ƒë·ªÅ: AI trong Gi√°o d·ª•c\", \n",
    "                   className=\"text-center mb-4\",\n",
    "                   style={'color': '#7f8c8d'})\n",
    "        ])\n",
    "    ]),\n",
    "    \n",
    "    # Summary Cards\n",
    "    dbc.Row([\n",
    "        dbc.Col([\n",
    "            dbc.Card([\n",
    "                dbc.CardBody([\n",
    "                    html.H4(\"üìù T·ªïng Posts\", className=\"card-title\"),\n",
    "                    html.H2(f\"{len(df):,}\", style={'color': '#3498db'})\n",
    "                ])\n",
    "            ], style={'backgroundColor': COLORS['card']})\n",
    "        ], width=3),\n",
    "        \n",
    "        dbc.Col([\n",
    "            dbc.Card([\n",
    "                dbc.CardBody([\n",
    "                    html.H4(\"üòä T√≠ch c·ª±c\", className=\"card-title\"),\n",
    "                    html.H2(f\"{len(df[df['sentiment']=='positive']):,}\", \n",
    "                           style={'color': COLORS['positive']})\n",
    "                ])\n",
    "            ], style={'backgroundColor': COLORS['card']})\n",
    "        ], width=3),\n",
    "        \n",
    "        dbc.Col([\n",
    "            dbc.Card([\n",
    "                dbc.CardBody([\n",
    "                    html.H4(\"üòû Ti√™u c·ª±c\", className=\"card-title\"),\n",
    "                    html.H2(f\"{len(df[df['sentiment']=='negative']):,}\", \n",
    "                           style={'color': COLORS['negative']})\n",
    "                ])\n",
    "            ], style={'backgroundColor': COLORS['card']})\n",
    "        ], width=3),\n",
    "        \n",
    "        dbc.Col([\n",
    "            dbc.Card([\n",
    "                dbc.CardBody([\n",
    "                    html.H4(\"üòê Trung l·∫≠p\", className=\"card-title\"),\n",
    "                    html.H2(f\"{len(df[df['sentiment']=='neutral']):,}\", \n",
    "                           style={'color': COLORS['neutral']})\n",
    "                ])\n",
    "            ], style={'backgroundColor': COLORS['card']})\n",
    "        ], width=3),\n",
    "    ], className=\"mb-4\"),\n",
    "    \n",
    "    # Charts Row 1\n",
    "    dbc.Row([\n",
    "        dbc.Col([\n",
    "            dbc.Card([\n",
    "                dbc.CardHeader(\"üìà Xu h∆∞·ªõng th·∫£o lu·∫≠n theo th·ªùi gian\"),\n",
    "                dbc.CardBody([\n",
    "                    dcc.Graph(id='timeline-chart')\n",
    "                ])\n",
    "            ])\n",
    "        ], width=8),\n",
    "        \n",
    "        dbc.Col([\n",
    "            dbc.Card([\n",
    "                dbc.CardHeader(\"üéØ Ph√¢n b·ªë c·∫£m x√∫c\"),\n",
    "                dbc.CardBody([\n",
    "                    dcc.Graph(id='sentiment-pie')\n",
    "                ])\n",
    "            ])\n",
    "        ], width=4),\n",
    "    ], className=\"mb-4\"),\n",
    "    \n",
    "    # Charts Row 2\n",
    "    dbc.Row([\n",
    "        dbc.Col([\n",
    "            dbc.Card([\n",
    "                dbc.CardHeader(\"üî• Top 15 Hashtags\"),\n",
    "                dbc.CardBody([\n",
    "                    dcc.Graph(id='hashtag-chart')\n",
    "                ])\n",
    "            ])\n",
    "        ], width=6),\n",
    "        \n",
    "        dbc.Col([\n",
    "            dbc.Card([\n",
    "                dbc.CardHeader(\"üìä C·∫£m x√∫c theo ch·ªß ƒë·ªÅ\"),\n",
    "                dbc.CardBody([\n",
    "                    dcc.Graph(id='topic-sentiment-chart')\n",
    "                ])\n",
    "            ])\n",
    "        ], width=6),\n",
    "    ], className=\"mb-4\"),\n",
    "    \n",
    "    # Charts Row 3\n",
    "    dbc.Row([\n",
    "        dbc.Col([\n",
    "            dbc.Card([\n",
    "                dbc.CardHeader(\"üìÖ Ph√¢n t√≠ch theo th√°ng\"),\n",
    "                dbc.CardBody([\n",
    "                    dcc.Graph(id='monthly-trend')\n",
    "                ])\n",
    "            ])\n",
    "        ], width=12),\n",
    "    ], className=\"mb-4\"),\n",
    "    \n",
    "    # Charts Row 4\n",
    "    dbc.Row([\n",
    "        dbc.Col([\n",
    "            dbc.Card([\n",
    "                dbc.CardHeader(\"üí¨ WordCloud - T·ª´ kh√≥a ph·ªï bi·∫øn\"),\n",
    "                dbc.CardBody([\n",
    "                    html.Img(id='wordcloud-img', style={'width': '100%'})\n",
    "                ])\n",
    "            ])\n",
    "        ], width=6),\n",
    "        \n",
    "        dbc.Col([\n",
    "            dbc.Card([\n",
    "                dbc.CardHeader(\"üé≠ Ph√¢n t√≠ch chi ti·∫øt c·∫£m x√∫c\"),\n",
    "                dbc.CardBody([\n",
    "                    dcc.Graph(id='sentiment-detail')\n",
    "                ])\n",
    "            ])\n",
    "        ], width=6),\n",
    "    ], className=\"mb-4\"),\n",
    "    \n",
    "], fluid=True, style={'backgroundColor': COLORS['background']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13f7ff4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting Dashboard...\n",
      "üìç Dashboard URL: http://127.0.0.1:8050\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x20059577430>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cell 10: Dashboard Callbacks\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import base64\n",
    "\n",
    "@app.callback(\n",
    "    Output('timeline-chart', 'figure'),\n",
    "    Input('timeline-chart', 'id')\n",
    ")\n",
    "def update_timeline(id):\n",
    "    \"\"\"Bi·ªÉu ƒë·ªì xu h∆∞·ªõng theo th·ªùi gian\"\"\"\n",
    "    daily = df.groupby(['date', 'sentiment']).size().unstack(fill_value=0)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for sentiment in ['positive', 'negative', 'neutral']:\n",
    "        if sentiment in daily.columns:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=daily.index,\n",
    "                y=daily[sentiment],\n",
    "                name=sentiment.capitalize(),\n",
    "                mode='lines+markers',\n",
    "                line=dict(color=COLORS[sentiment], width=2),\n",
    "                fill='tonexty' if sentiment != 'positive' else None\n",
    "            ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Xu h∆∞·ªõng th·∫£o lu·∫≠n theo ng√†y\",\n",
    "        xaxis_title=\"Ng√†y\",\n",
    "        yaxis_title=\"S·ªë l∆∞·ª£ng posts\",\n",
    "        hovermode='x unified',\n",
    "        template='plotly_white',\n",
    "        height=400\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "@app.callback(\n",
    "    Output('sentiment-pie', 'figure'),\n",
    "    Input('sentiment-pie', 'id')\n",
    ")\n",
    "def update_sentiment_pie(id):\n",
    "    \"\"\"Bi·ªÉu ƒë·ªì tr√≤n ph√¢n b·ªë c·∫£m x√∫c\"\"\"\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "    \n",
    "    fig = go.Figure(data=[go.Pie(\n",
    "        labels=[s.capitalize() for s in sentiment_counts.index],\n",
    "        values=sentiment_counts.values,\n",
    "        marker=dict(colors=[COLORS[s] for s in sentiment_counts.index]),\n",
    "        hole=0.4,\n",
    "        textinfo='label+percent',\n",
    "        textfont_size=12\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"T·ª∑ l·ªá c·∫£m x√∫c\",\n",
    "        height=400,\n",
    "        template='plotly_white',\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "@app.callback(\n",
    "    Output('hashtag-chart', 'figure'),\n",
    "    Input('hashtag-chart', 'id')\n",
    ")\n",
    "def update_hashtag_chart(id):\n",
    "    \"\"\"Bi·ªÉu ƒë·ªì top hashtags\"\"\"\n",
    "    top_hashtags = trend_data.get('top_hashtags', {}) if trend_data else {}\n",
    "    \n",
    "    if not top_hashtags:\n",
    "        # Fallback: t√≠nh t·ª´ dataframe\n",
    "        all_hashtags = []\n",
    "        for hashtags in df['hashtags'].dropna():\n",
    "            if isinstance(hashtags, list):\n",
    "                all_hashtags.extend(hashtags)\n",
    "        from collections import Counter\n",
    "        top_hashtags = dict(Counter(all_hashtags).most_common(15))\n",
    "    \n",
    "    hashtags = list(top_hashtags.keys())[:15]\n",
    "    counts = list(top_hashtags.values())[:15]\n",
    "    \n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            x=counts,\n",
    "            y=hashtags,\n",
    "            orientation='h',\n",
    "            marker=dict(\n",
    "                color=counts,\n",
    "                colorscale='Viridis',\n",
    "                showscale=True\n",
    "            ),\n",
    "            text=counts,\n",
    "            textposition='auto'\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Top 15 Hashtags ph·ªï bi·∫øn nh·∫•t\",\n",
    "        xaxis_title=\"S·ªë l∆∞·ª£ng\",\n",
    "        yaxis_title=\"Hashtag\",\n",
    "        height=400,\n",
    "        template='plotly_white',\n",
    "        yaxis={'categoryorder': 'total ascending'}\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "@app.callback(\n",
    "    Output('topic-sentiment-chart', 'figure'),\n",
    "    Input('topic-sentiment-chart', 'id')\n",
    ")\n",
    "def update_topic_sentiment(id):\n",
    "    \"\"\"Bi·ªÉu ƒë·ªì c·∫£m x√∫c theo ch·ªß ƒë·ªÅ\"\"\"\n",
    "    topic_sentiment = df.groupby(['topic', 'sentiment']).size().unstack(fill_value=0)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for sentiment in ['positive', 'neutral', 'negative']:\n",
    "        if sentiment in topic_sentiment.columns:\n",
    "            fig.add_trace(go.Bar(\n",
    "                name=sentiment.capitalize(),\n",
    "                x=topic_sentiment.index,\n",
    "                y=topic_sentiment[sentiment],\n",
    "                marker_color=COLORS[sentiment]\n",
    "            ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Ph√¢n b·ªë c·∫£m x√∫c theo ch·ªß ƒë·ªÅ\",\n",
    "        xaxis_title=\"Ch·ªß ƒë·ªÅ\",\n",
    "        yaxis_title=\"S·ªë l∆∞·ª£ng posts\",\n",
    "        barmode='stack',\n",
    "        height=400,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "@app.callback(\n",
    "    Output('monthly-trend', 'figure'),\n",
    "    Input('monthly-trend', 'id')\n",
    ")\n",
    "def update_monthly_trend(id):\n",
    "    \"\"\"Bi·ªÉu ƒë·ªì ph√¢n t√≠ch theo th√°ng\"\"\"\n",
    "    monthly = df.groupby('month').agg({\n",
    "        'text': 'count',\n",
    "        'sentiment_score': 'mean',\n",
    "        'likes': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    monthly.columns = ['month', 'post_count', 'avg_sentiment', 'total_likes']\n",
    "    \n",
    "    # T·∫°o subplot v·ªõi 2 tr·ª•c y\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=1,\n",
    "        specs=[[{\"secondary_y\": True}]]\n",
    "    )\n",
    "    \n",
    "    # S·ªë l∆∞·ª£ng posts\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=monthly['month'],\n",
    "            y=monthly['post_count'],\n",
    "            name='S·ªë posts',\n",
    "            marker_color='#3498db',\n",
    "            yaxis='y'\n",
    "        ),\n",
    "        secondary_y=False\n",
    "    )\n",
    "    \n",
    "    # ƒêi·ªÉm c·∫£m x√∫c trung b√¨nh\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=monthly['month'],\n",
    "            y=monthly['avg_sentiment'],\n",
    "            name='ƒêi·ªÉm c·∫£m x√∫c TB',\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='#e74c3c', width=3),\n",
    "            marker=dict(size=8),\n",
    "            yaxis='y2'\n",
    "        ),\n",
    "        secondary_y=True\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Th√°ng\")\n",
    "    fig.update_yaxes(title_text=\"S·ªë l∆∞·ª£ng posts\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"ƒêi·ªÉm c·∫£m x√∫c trung b√¨nh\", secondary_y=True)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Xu h∆∞·ªõng theo th√°ng\",\n",
    "        height=400,\n",
    "        template='plotly_white',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "@app.callback(\n",
    "    Output('wordcloud-img', 'src'),\n",
    "    Input('wordcloud-img', 'id')\n",
    ")\n",
    "def update_wordcloud(id):\n",
    "    \"\"\"T·∫°o WordCloud\"\"\"\n",
    "    # L·∫•y t·∫•t c·∫£ text\n",
    "    text = ' '.join(df['text'].dropna().astype(str))\n",
    "    \n",
    "    # T·∫°o wordcloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        colormap='viridis',\n",
    "        max_words=100,\n",
    "        relative_scaling=0.5,\n",
    "        min_font_size=10\n",
    "    ).generate(text)\n",
    "    \n",
    "    # Convert to image\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    # Save to bytes\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', bbox_inches='tight', dpi=100)\n",
    "    buf.seek(0)\n",
    "    plt.close()\n",
    "    \n",
    "    # Encode to base64\n",
    "    encoded = base64.b64encode(buf.read()).decode('utf-8')\n",
    "    \n",
    "    return f'data:image/png;base64,{encoded}'\n",
    "\n",
    "@app.callback(\n",
    "    Output('sentiment-detail', 'figure'),\n",
    "    Input('sentiment-detail', 'id')\n",
    ")\n",
    "def update_sentiment_detail(id):\n",
    "    \"\"\"Ph√¢n t√≠ch chi ti·∫øt ƒëi·ªÉm c·∫£m x√∫c\"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for sentiment in ['positive', 'neutral', 'negative']:\n",
    "        sentiment_data = df[df['sentiment'] == sentiment]['sentiment_score']\n",
    "        \n",
    "        fig.add_trace(go.Box(\n",
    "            y=sentiment_data,\n",
    "            name=sentiment.capitalize(),\n",
    "            marker_color=COLORS[sentiment],\n",
    "            boxmean='sd'\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Ph√¢n b·ªë ƒëi·ªÉm c·∫£m x√∫c (Sentiment Score)\",\n",
    "        yaxis_title=\"ƒêi·ªÉm c·∫£m x√∫c (-1 ƒë·∫øn 1)\",\n",
    "        height=400,\n",
    "        template='plotly_white',\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Ch·∫°y app\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\nüöÄ Starting Dashboard...\")\n",
    "    print(\"üìç Dashboard URL: http://127.0.0.1:8050\")\n",
    "    app.run_server(debug=True, port=8050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c60c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä COMPREHENSIVE SOCIAL MEDIA ANALYSIS REPORT\n",
      "==================================================\n",
      "\n",
      "üìà Basic Statistics:\n",
      "Total Posts: 1600\n",
      "Unique Topics: 6\n",
      "Date Range: 2025-07-17 23:22:59.207000 to 2025-10-15 23:22:59.229000\n",
      "\n",
      "üí≠ Sentiment Analysis:\n",
      "Distribution:\n",
      "  Negative: 682 posts (42.62%)\n",
      "  Neutral: 619 posts (38.69%)\n",
      "  Positive: 299 posts (18.69%)\n",
      "Average Sentiment Score: -0.135\n",
      "\n",
      "üìÖ Time Analysis:\n",
      "Posts by Day of Week:\n",
      "  Wednesday: 655 posts\n",
      "  Tuesday: 244 posts\n",
      "  Thursday: 154 posts\n",
      "\n",
      "üî• Top Hashtags:\n",
      "  #Tr√≠Tu·ªáNh√¢nT·∫°o: 262 mentions\n",
      "  #C√¥ngNgh·ªáGi√°oD·ª•c: 258 mentions\n",
      "  #AIEducation: 256 mentions\n",
      "  #Gi√°oD·ª•c: 252 mentions\n",
      "  #H·ªçcT·∫≠pAI: 249 mentions\n",
      "\n",
      "üìä Engagement Statistics:\n",
      "  Likes:\n",
      "    Average: 504.21\n",
      "    Max: 1000\n",
      "  Retweets:\n",
      "    Average: 251.63\n",
      "    Max: 500\n",
      "  Replies:\n",
      "    Average: 100.44\n",
      "    Max: 200\n",
      "  Score:\n",
      "    Average: 5.30\n",
      "    Max: 386\n",
      "\n",
      "‚úÖ Analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 Fixed - Statistical Analysis\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load data from MongoDB\n",
    "df = pd.DataFrame(list(posts_collection.find()))\n",
    "\n",
    "class StatisticalAnalyzer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        # Add day_of_week column if it doesn't exist\n",
    "        if 'created_at' in self.df.columns and 'day_of_week' not in self.df.columns:\n",
    "            self.df['day_of_week'] = pd.to_datetime(self.df['created_at']).dt.day_name()\n",
    "    \n",
    "    def get_basic_stats(self):\n",
    "        \"\"\"Th·ªëng k√™ c∆° b·∫£n\"\"\"\n",
    "        return {\n",
    "            'total_posts': len(self.df),\n",
    "            'unique_topics': self.df['topic'].nunique() if 'topic' in self.df.columns else 0,\n",
    "            'date_range': {\n",
    "                'start': str(self.df['created_at'].min()) if 'created_at' in self.df.columns else None,\n",
    "                'end': str(self.df['created_at'].max()) if 'created_at' in self.df.columns else None\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_sentiment_stats(self):\n",
    "        \"\"\"Th·ªëng k√™ c·∫£m x√∫c\"\"\"\n",
    "        if 'sentiment' not in self.df.columns:\n",
    "            return {'error': 'No sentiment data available'}\n",
    "        \n",
    "        sentiment_counts = self.df['sentiment'].value_counts()\n",
    "        total = len(self.df)\n",
    "        \n",
    "        return {\n",
    "            'distribution': sentiment_counts.to_dict(),\n",
    "            'percentages': (sentiment_counts / total * 100).round(2).to_dict(),\n",
    "            'avg_sentiment_score': float(self.df['sentiment_score'].mean()) if 'sentiment_score' in self.df.columns else None\n",
    "        }\n",
    "    \n",
    "    def get_engagement_stats(self):\n",
    "        \"\"\"Th·ªëng k√™ t∆∞∆°ng t√°c\"\"\"\n",
    "        engagement_cols = ['likes', 'retweets', 'replies', 'score']\n",
    "        stats = {}\n",
    "        \n",
    "        for col in engagement_cols:\n",
    "            if col in self.df.columns:\n",
    "                stats[col] = {\n",
    "                    'mean': float(self.df[col].mean()),\n",
    "                    'median': float(self.df[col].median()),\n",
    "                    'max': int(self.df[col].max()),\n",
    "                    'min': int(self.df[col].min())\n",
    "                }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def get_time_analysis(self):\n",
    "        \"\"\"Ph√¢n t√≠ch theo th·ªùi gian\"\"\"\n",
    "        if 'created_at' not in self.df.columns:\n",
    "            return {'error': 'No time data available'}\n",
    "        \n",
    "        # Ensure day_of_week exists\n",
    "        if 'day_of_week' not in self.df.columns:\n",
    "            self.df['day_of_week'] = pd.to_datetime(self.df['created_at']).dt.day_name()\n",
    "        \n",
    "        return {\n",
    "            'posts_by_day': self.df['day_of_week'].value_counts().to_dict(),\n",
    "            'posts_by_hour': pd.to_datetime(self.df['created_at']).dt.hour.value_counts().to_dict()\n",
    "        }\n",
    "    \n",
    "    def get_top_hashtags(self, limit=10):\n",
    "        \"\"\"Top hashtags\"\"\"\n",
    "        if 'hashtags' not in self.df.columns:\n",
    "            return []\n",
    "        \n",
    "        all_hashtags = []\n",
    "        for hashtags in self.df['hashtags'].dropna():\n",
    "            if isinstance(hashtags, list):\n",
    "                all_hashtags.extend(hashtags)\n",
    "        \n",
    "        return Counter(all_hashtags).most_common(limit)\n",
    "    \n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"T·∫°o b√°o c√°o th·ªëng k√™ to√†n di·ªán\"\"\"\n",
    "        report = {\n",
    "            'basic_stats': self.get_basic_stats(),\n",
    "            'sentiment_stats': self.get_sentiment_stats(),\n",
    "            'time_analysis': self.get_time_analysis(),\n",
    "            'engagement_stats': self.get_engagement_stats(),\n",
    "            'top_hashtags': self.get_top_hashtags(),\n",
    "            'generated_at': datetime.now()\n",
    "        }\n",
    "        return report\n",
    "\n",
    "# T·∫°o b√°o c√°o\n",
    "stat_analyzer = StatisticalAnalyzer(df)\n",
    "comprehensive_report = stat_analyzer.generate_comprehensive_report()\n",
    "\n",
    "# In b√°o c√°o\n",
    "print(\"üìä COMPREHENSIVE SOCIAL MEDIA ANALYSIS REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüìà Basic Statistics:\")\n",
    "basic = comprehensive_report['basic_stats']\n",
    "print(f\"Total Posts: {basic['total_posts']}\")\n",
    "print(f\"Unique Topics: {basic['unique_topics']}\")\n",
    "print(f\"Date Range: {basic['date_range']['start']} to {basic['date_range']['end']}\")\n",
    "\n",
    "print(f\"\\nüí≠ Sentiment Analysis:\")\n",
    "sentiment = comprehensive_report['sentiment_stats']\n",
    "if 'error' not in sentiment:\n",
    "    print(\"Distribution:\")\n",
    "    for sentiment_type, count in sentiment['distribution'].items():\n",
    "        pct = sentiment['percentages'][sentiment_type]\n",
    "        print(f\"  {sentiment_type.title()}: {count} posts ({pct}%)\")\n",
    "    print(f\"Average Sentiment Score: {sentiment['avg_sentiment_score']:.3f}\")\n",
    "\n",
    "print(f\"\\nüìÖ Time Analysis:\")\n",
    "time_analysis = comprehensive_report['time_analysis']\n",
    "if 'error' not in time_analysis:\n",
    "    print(\"Posts by Day of Week:\")\n",
    "    for day, count in sorted(time_analysis['posts_by_day'].items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "        print(f\"  {day}: {count} posts\")\n",
    "\n",
    "print(f\"\\nüî• Top Hashtags:\")\n",
    "for hashtag, count in comprehensive_report['top_hashtags'][:5]:\n",
    "    print(f\"  #{hashtag}: {count} mentions\")\n",
    "\n",
    "print(f\"\\nüìä Engagement Statistics:\")\n",
    "engagement = comprehensive_report['engagement_stats']\n",
    "for metric, stats in engagement.items():\n",
    "    print(f\"  {metric.title()}:\")\n",
    "    print(f\"    Average: {stats['mean']:.2f}\")\n",
    "    print(f\"    Max: {stats['max']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06f0c3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ PH√ÇN T√çCH N√ÇNG CAO\n",
      "\n",
      "üìö TOPIC MODELING (LDA):\n",
      "\n",
      "  Topic 0:\n",
      "  Top words: ai, free, tools, growth, data, market, content, 2025, company, education\n",
      "\n",
      "  Topic 1:\n",
      "  Top words: https, com, www, png, intelligence, openai, news, 10, nse, models\n",
      "\n",
      "  Topic 2:\n",
      "  Top words: trong, edtech, h·ªçc, gi√°o, d·ª•c, ai, h·ªçct·∫≠pai, ƒë·∫°i, l·ªõp, machinelearning\n",
      "\n",
      "  Topic 3:\n",
      "  Top words: h·ªçc, t·∫°o, trong, m√°y, tu·ªá, tr√≠, nh√¢n, c√¥ngngh·ªági√°od·ª•c, tr∆∞·ªùng, machinelearning\n",
      "\n",
      "  Topic 4:\n",
      "  Top words: just, ai, like, tech, time, work, right, help, know, make\n",
      "\n",
      "üìä CORRELATION ANALYSIS:\n",
      "                 sentiment_score     likes  retweets   replies\n",
      "sentiment_score         1.000000  0.023084 -0.052486  0.025731\n",
      "likes                   0.023084  1.000000  0.039456 -0.046213\n",
      "retweets               -0.052486  0.039456  1.000000  0.033473\n",
      "replies                 0.025731 -0.046213  0.033473  1.000000\n",
      "\n",
      "üîë TOP 15 KEYWORDS (TF-IDF):\n",
      "  1. ai: 0.5952\n",
      "  2. https: 0.4096\n",
      "  3. com: 0.3207\n",
      "  4. trong: 0.2801\n",
      "  5. h·ªçc: 0.2799\n",
      "  6. data: 0.1842\n",
      "  7. like: 0.1833\n",
      "  8. content: 0.1793\n",
      "  9. just: 0.1565\n",
      "  10. edtech: 0.1347\n",
      "  11. time: 0.1340\n",
      "  12. company: 0.1329\n",
      "  13. www: 0.1306\n",
      "  14. new: 0.1083\n",
      "  15. 2025: 0.1079\n",
      "\n",
      "üí™ SENTIMENT BY ENGAGEMENT LEVEL:\n",
      "sentiment         negative  neutral  positive\n",
      "engagement_level                             \n",
      "Low                      8        3         0\n",
      "Medium                  16       10         3\n",
      "High                    22        7        10\n",
      "Viral                  558      217       146\n"
     ]
    }
   ],
   "source": [
    "# cell 12: Advanced Analysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AdvancedAnalyzer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def topic_modeling(self, n_topics=5):\n",
    "        \"\"\"Topic Modeling v·ªõi LDA\"\"\"\n",
    "        texts = self.df['text'].fillna('').tolist()\n",
    "        \n",
    "        # TF-IDF\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=100,\n",
    "            min_df=2,\n",
    "            max_df=0.8,\n",
    "            stop_words='english'\n",
    "        )\n",
    "        \n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        # LDA\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=n_topics,\n",
    "            random_state=42,\n",
    "            max_iter=10\n",
    "        )\n",
    "        \n",
    "        lda.fit(tfidf_matrix)\n",
    "        \n",
    "        # L·∫•y top words cho m·ªói topic\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        topics = []\n",
    "        \n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            top_indices = topic.argsort()[-10:][::-1]\n",
    "            top_words = [feature_names[i] for i in top_indices]\n",
    "            topics.append({\n",
    "                'topic_id': topic_idx,\n",
    "                'top_words': top_words\n",
    "            })\n",
    "        \n",
    "        return topics\n",
    "    \n",
    "    def sentiment_correlation(self):\n",
    "        \"\"\"Ph√¢n t√≠ch correlation gi·ªØa sentiment v√† engagement\"\"\"\n",
    "        correlation = self.df[['sentiment_score', 'likes', 'retweets', 'replies']].corr()\n",
    "        return correlation\n",
    "    \n",
    "    def time_series_forecast(self):\n",
    "        \"\"\"D·ª± b√°o xu h∆∞·ªõng ƒë∆°n gi·∫£n\"\"\"\n",
    "        daily = self.df.groupby('date').size().reset_index()\n",
    "        daily.columns = ['date', 'count']\n",
    "        \n",
    "        # Simple moving average\n",
    "        daily['MA_7'] = daily['count'].rolling(window=7).mean()\n",
    "        daily['MA_30'] = daily['count'].rolling(window=30).mean()\n",
    "        \n",
    "        return daily\n",
    "    \n",
    "    def sentiment_by_engagement(self):\n",
    "        \"\"\"Ph√¢n t√≠ch c·∫£m x√∫c theo m·ª©c ƒë·ªô engagement\"\"\"\n",
    "        # Ph√¢n lo·∫°i engagement\n",
    "        self.df['engagement_level'] = pd.cut(\n",
    "            self.df['likes'],\n",
    "            bins=[0, 10, 50, 100, float('inf')],\n",
    "            labels=['Low', 'Medium', 'High', 'Viral']\n",
    "        )\n",
    "        \n",
    "        engagement_sentiment = self.df.groupby(['engagement_level', 'sentiment']).size().unstack(fill_value=0)\n",
    "        \n",
    "        return engagement_sentiment\n",
    "    \n",
    "    def keyword_extraction(self, top_n=20):\n",
    "        \"\"\"Tr√≠ch xu·∫•t t·ª´ kh√≥a quan tr·ªçng\"\"\"\n",
    "        texts = ' '.join(self.df['text'].fillna(''))\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(max_features=top_n, stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform([texts])\n",
    "        \n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        scores = tfidf_matrix.toarray()[0]\n",
    "        \n",
    "        keywords = sorted(zip(feature_names, scores), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return keywords\n",
    "\n",
    "# Ch·∫°y ph√¢n t√≠ch n√¢ng cao\n",
    "advanced_analyzer = AdvancedAnalyzer(df)\n",
    "\n",
    "print(\"\\nüî¨ PH√ÇN T√çCH N√ÇNG CAO\\n\")\n",
    "\n",
    "print(\"üìö TOPIC MODELING (LDA):\")\n",
    "topics = advanced_analyzer.topic_modeling(n_topics=5)\n",
    "for topic in topics:\n",
    "    print(f\"\\n  Topic {topic['topic_id']}:\")\n",
    "    print(f\"  Top words: {', '.join(topic['top_words'][:10])}\")\n",
    "\n",
    "print(\"\\nüìä CORRELATION ANALYSIS:\")\n",
    "correlation = advanced_analyzer.sentiment_correlation()\n",
    "print(correlation)\n",
    "\n",
    "print(\"\\nüîë TOP 15 KEYWORDS (TF-IDF):\")\n",
    "keywords = advanced_analyzer.keyword_extraction(15)\n",
    "for i, (word, score) in enumerate(keywords, 1):\n",
    "    print(f\"  {i}. {word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nüí™ SENTIMENT BY ENGAGEMENT LEVEL:\")\n",
    "engagement_sentiment = advanced_analyzer.sentiment_by_engagement()\n",
    "print(engagement_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99a3322d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì§ EXPORTING RESULTS...\n",
      "\n",
      "‚úÖ Exported to social_media_analysis_20251015_232813.csv\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 106\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müì§ EXPORTING RESULTS...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    105\u001b[0m exporter\u001b[38;5;241m.\u001b[39mexport_to_csv()\n\u001b[1;32m--> 106\u001b[0m \u001b[43mexporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_to_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m exporter\u001b[38;5;241m.\u001b[39mexport_to_json()\n\u001b[0;32m    108\u001b[0m exporter\u001b[38;5;241m.\u001b[39mcreate_presentation_summary()\n",
      "Cell \u001b[1;32mIn[44], line 22\u001b[0m, in \u001b[0;36mReportExporter.export_to_excel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Export to Excel with multiple sheets\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msocial_media_report_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopenpyxl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Main data\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mto_excel(writer, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRaw Data\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Summary statistics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\txtru\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:57\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     46\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m WriteExcelBuffer \u001b[38;5;241m|\u001b[39m ExcelWriter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[0;32m     59\u001b[0m     engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     62\u001b[0m         path,\n\u001b[0;32m     63\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m     67\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "# cell 13: Export Results\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class ReportExporter:\n",
    "    def __init__(self, df, report_data):\n",
    "        self.df = df\n",
    "        self.report_data = report_data\n",
    "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    def export_to_csv(self):\n",
    "        \"\"\"Export DataFrame to CSV\"\"\"\n",
    "        filename = f'social_media_analysis_{self.timestamp}.csv'\n",
    "        self.df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"‚úÖ Exported to {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def export_to_excel(self):\n",
    "        \"\"\"Export to Excel with multiple sheets\"\"\"\n",
    "        filename = f'social_media_report_{self.timestamp}.xlsx'\n",
    "        \n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            # Main data\n",
    "            self.df.to_excel(writer, sheet_name='Raw Data', index=False)\n",
    "            \n",
    "            # Summary statistics\n",
    "            summary = self.df.groupby('sentiment').agg({\n",
    "                'text': 'count',\n",
    "                'likes': 'sum',\n",
    "                'sentiment_score': 'mean'\n",
    "            })\n",
    "            summary.to_excel(writer, sheet_name='Summary')\n",
    "            \n",
    "            # Topic analysis\n",
    "            topic_df = pd.DataFrame(self.report_data['topic_analysis'])\n",
    "            topic_df.to_excel(writer, sheet_name='Topic Analysis', index=False)\n",
    "            \n",
    "            # Daily trend\n",
    "            daily_trend = self.df.groupby('date').agg({\n",
    "                'text': 'count',\n",
    "                'likes': 'sum',\n",
    "                'sentiment_score': 'mean'\n",
    "            })\n",
    "            daily_trend.to_excel(writer, sheet_name='Daily Trend')\n",
    "        \n",
    "        print(f\"‚úÖ Exported to {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def export_to_json(self):\n",
    "        \"\"\"Export report to JSON\"\"\"\n",
    "        filename = f'analysis_report_{self.timestamp}.json'\n",
    "        \n",
    "        report = {\n",
    "            'metadata': {\n",
    "                'generated_at': datetime.now().isoformat(),\n",
    "                'total_records': len(self.df),\n",
    "                'date_range': {\n",
    "                    'start': self.df['created_at'].min().isoformat(),\n",
    "                    'end': self.df['created_at'].max().isoformat()\n",
    "                }\n",
    "            },\n",
    "            'analysis': self.report_data\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=2, default=str)\n",
    "        \n",
    "        print(f\"‚úÖ Exported to {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def create_presentation_summary(self):\n",
    "        \"\"\"T·∫°o file summary cho presentation\"\"\"\n",
    "        filename = f'presentation_summary_{self.timestamp}.txt'\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"SOCIAL MEDIA ANALYSIS SUMMARY\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Dataset Size: {len(self.df):,} posts\\n\")\n",
    "            f.write(f\"Date Range: {self.df['created_at'].min().date()} to {self.df['created_at'].max().date()}\\n\\n\")\n",
    "            \n",
    "            f.write(\"SENTIMENT DISTRIBUTION:\\n\")\n",
    "            sentiment_counts = self.df['sentiment'].value_counts()\n",
    "            for sentiment, count in sentiment_counts.items():\n",
    "                pct = count / len(self.df) * 100\n",
    "                f.write(f\"  - {sentiment.capitalize()}: {count:,} ({pct:.1f}%)\\n\")\n",
    "            \n",
    "            f.write(\"\\nTOP 5 TOPICS:\\n\")\n",
    "            for i, topic in enumerate(self.report_data['topic_analysis'][:5], 1):\n",
    "                f.write(f\"  {i}. {topic['topic']}: {topic['total_posts']:,} posts\\n\")\n",
    "            \n",
    "            f.write(\"\\nTOP 10 HASHTAGS:\\n\")\n",
    "            hashtag_items = list(self.report_data['hashtag_analysis'].items())[:10]\n",
    "            for i, (tag, data) in enumerate(hashtag_items, 1):\n",
    "                f.write(f\"  {i}. #{tag}: {data['count']} mentions\\n\")\n",
    "        \n",
    "        print(f\"‚úÖ Created summary: {filename}\")\n",
    "        return filename\n",
    "\n",
    "# Export results\n",
    "exporter = ReportExporter(df, comprehensive_report)\n",
    "\n",
    "print(\"\\nüì§ EXPORTING RESULTS...\\n\")\n",
    "exporter.export_to_csv()\n",
    "exporter.export_to_excel()\n",
    "exporter.export_to_json()\n",
    "exporter.create_presentation_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4242d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 14: Streamlit Dashboard (save as streamlit_dashboard.py)\n",
    "\"\"\"\n",
    "Ch·∫°y file n√†y ri√™ng v·ªõi l·ªánh: streamlit run streamlit_dashboard.py\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pymongo import MongoClient\n",
    "import sys\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Social Media Analytics Dashboard\",\n",
    "    page_icon=\"üìä\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Custom CSS\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    .main {\n",
    "        background-color: #f8f9fa;\n",
    "    }\n",
    "    .stMetric {\n",
    "        background-color: white;\n",
    "        padding: 15px;\n",
    "        border-radius: 10px;\n",
    "        box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "    }\n",
    "    </style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Load data\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client['social_media_analysis']\n",
    "    posts = list(db['posts'].find())\n",
    "    df = pd.DataFrame(posts)\n",
    "    \n",
    "    if 'created_at' in df.columns:\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        df['date'] = df['created_at'].dt.date\n",
    "        df['month'] = df['created_at'].dt.to_period('M').astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "# Sidebar\n",
    "st.sidebar.title(\"üéõÔ∏è B·ªô l·ªçc\")\n",
    "\n",
    "# Date range filter\n",
    "if 'created_at' in df.columns:\n",
    "    min_date = df['created_at'].min().date()\n",
    "    max_date = df['created_at'].max().date()\n",
    "    \n",
    "    date_range = st.sidebar.date_input(\n",
    "        \"Ch·ªçn kho·∫£ng th·ªùi gian\",\n",
    "        value=(min_date, max_date),\n",
    "        min_value=min_date,\n",
    "        max_value=max_date\n",
    "    )\n",
    "    \n",
    "    if len(date_range) == 2:\n",
    "        mask = (df['created_at'].dt.date >= date_range[0]) & (df['created_at'].dt.date <= date_range[1])\n",
    "        df_filtered = df[mask]\n",
    "    else:\n",
    "        df_filtered = df\n",
    "else:\n",
    "    df_filtered = df\n",
    "\n",
    "# Sentiment filter\n",
    "sentiment_filter = st.sidebar.multiselect(\n",
    "    \"C·∫£m x√∫c\",\n",
    "    options=['positive', 'negative', 'neutral'],\n",
    "    default=['positive', 'negative', 'neutral']\n",
    ")\n",
    "\n",
    "df_filtered = df_filtered[df_filtered['sentiment'].isin(sentiment_filter)]\n",
    "\n",
    "# Topic filter\n",
    "if 'topic' in df_filtered.columns:\n",
    "    topics = df_filtered['topic'].unique().tolist()\n",
    "    selected_topics = st.sidebar.multiselect(\n",
    "        \"Ch·ªß ƒë·ªÅ\",\n",
    "        options=topics,\n",
    "        default=topics\n",
    "    )\n",
    "    df_filtered = df_filtered[df_filtered['topic'].isin(selected_topics)]\n",
    "\n",
    "# Main content\n",
    "st.title(\"üìä Dashboard Ph√¢n t√≠ch D·ªØ li·ªáu X√£ h·ªôi\")\n",
    "st.markdown(\"### Ch·ªß ƒë·ªÅ: AI trong Gi√°o d·ª•c\")\n",
    "\n",
    "# Metrics\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "\n",
    "with col1:\n",
    "    st.metric(\"üìù T·ªïng Posts\", f\"{len(df_filtered):,}\")\n",
    "\n",
    "with col2:\n",
    "    positive_count = len(df_filtered[df_filtered['sentiment'] == 'positive'])\n",
    "    st.metric(\"üòä T√≠ch c·ª±c\", f\"{positive_count:,}\", delta=f\"{positive_count/len(df_filtered)*100:.1f}%\")\n",
    "\n",
    "with col3:\n",
    "    negative_count = len(df_filtered[df_filtered['sentiment'] == 'negative'])\n",
    "    st.metric(\"üòû Ti√™u c·ª±c\", f\"{negative_count:,}\", delta=f\"{negative_count/len(df_filtered)*100:.1f}%\")\n",
    "\n",
    "with col4:\n",
    "    avg_sentiment = df_filtered['sentiment_score'].mean()\n",
    "    st.metric(\"üìà ƒêi·ªÉm TB\", f\"{avg_sentiment:.3f}\")\n",
    "\n",
    "st.markdown(\"---\")\n",
    "\n",
    "# Charts\n",
    "tab1, tab2, tab3, tab4 = st.tabs([\"üìà Xu h∆∞·ªõng\", \"üéØ C·∫£m x√∫c\", \"#Ô∏è‚É£ Hashtags\", \"üìä Th·ªëng k√™\"])\n",
    "\n",
    "with tab1:\n",
    "    st.subheader(\"Xu h∆∞·ªõng th·∫£o lu·∫≠n theo th·ªùi gian\")\n",
    "    \n",
    "    daily_data = df_filtered.groupby(['date', 'sentiment']).size().unstack(fill_value=0)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    for sentiment in ['positive', 'negative', 'neutral']:\n",
    "        if sentiment in daily_data.columns:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=daily_data.index,\n",
    "                y=daily_data[sentiment],\n",
    "                name=sentiment.capitalize(),\n",
    "                mode='lines+markers'\n",
    "            ))\n",
    "    \n",
    "    fig.update_layout(height=400, hovermode='x unified')\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "with tab2:\n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"Ph√¢n b·ªë c·∫£m x√∫c\")\n",
    "       sentiment_counts = df_filtered['sentiment'].value_counts()\n",
    "    \n",
    "    fig = px.pie(\n",
    "        values=sentiment_counts.values,\n",
    "        names=sentiment_counts.index,\n",
    "        color=sentiment_counts.index,\n",
    "        color_discrete_map={\n",
    "            'positive': '#2ecc71',\n",
    "            'negative': '#e74c3c',\n",
    "            'neutral': '#95a5a6'\n",
    "        },\n",
    "        hole=0.4\n",
    "    )\n",
    "    fig.update_layout(height=400)\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "with col2:\n",
    "    st.subheader(\"C·∫£m x√∫c theo ch·ªß ƒë·ªÅ\")\n",
    "    \n",
    "    topic_sentiment = df_filtered.groupby(['topic', 'sentiment']).size().unstack(fill_value=0)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    for sentiment in ['positive', 'neutral', 'negative']:\n",
    "        if sentiment in topic_sentiment.columns:\n",
    "            fig.add_trace(go.Bar(\n",
    "                name=sentiment.capitalize(),\n",
    "                x=topic_sentiment.index,\n",
    "                y=topic_sentiment[sentiment]\n",
    "            ))\n",
    "    \n",
    "    fig.update_layout(barmode='stack', height=400)\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "with tab3:\n",
    "    st.subheader(\"Top Hashtags ph·ªï bi·∫øn\")\n",
    "    all_hashtags = []\n",
    "for hashtags in df_filtered['hashtags'].dropna():\n",
    "    if isinstance(hashtags, list):\n",
    "        all_hashtags.extend(hashtags)\n",
    "\n",
    "from collections import Counter\n",
    "hashtag_counts = Counter(all_hashtags).most_common(20)\n",
    "\n",
    "hashtag_df = pd.DataFrame(hashtag_counts, columns=['Hashtag', 'Count'])\n",
    "\n",
    "fig = px.bar(\n",
    "    hashtag_df,\n",
    "    x='Count',\n",
    "    y='Hashtag',\n",
    "    orientation='h',\n",
    "    color='Count',\n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "fig.update_layout(height=600, yaxis={'categoryorder': 'total ascending'})\n",
    "st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "with tab4:\n",
    "    st.subheader(\"Th·ªëng k√™ chi ti·∫øt\")\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    st.markdown(\"#### Th·ªëng k√™ theo th√°ng\")\n",
    "    monthly = df_filtered.groupby('month').agg({\n",
    "        'text': 'count',\n",
    "        'sentiment_score': 'mean',\n",
    "        'likes': 'sum'\n",
    "    }).reset_index()\n",
    "    monthly.columns = ['Th√°ng', 'S·ªë posts', 'ƒêi·ªÉm CB', 'T·ªïng likes']\n",
    "    st.dataframe(monthly, use_container_width=True)\n",
    "\n",
    "with col2:\n",
    "    st.markdown(\"#### Top Posts c√≥ engagement cao\")\n",
    "    top_posts = df_filtered.nlargest(10, 'likes')[['text', 'likes', 'sentiment', 'created_at']]\n",
    "    top_posts['text'] = top_posts['text'].str[:100] + '...'\n",
    "    st.dataframe(top_posts, use_container_width=True)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
