# Twitter Data Collection v·ªõi snscrape (kh√¥ng c·∫ßn API key)
import snscrape.modules.twitter as sntwitter
import json
import re
from datetime import datetime, timedelta
from pymongo import MongoClient

class TwitterCrawler:
    def __init__(self):
        pass
    
    def search_tweets(self, query, max_results=100, since_date=None):
        """
        Thu th·∫≠p tweets v·ªÅ ch·ªß ƒë·ªÅ c·ª• th·ªÉ b·∫±ng snscrape
        """
        tweets_data = []
        
        try:
            # T·∫°o query v·ªõi ng√†y n·∫øu c√≥
            if since_date:
                search_query = f"{query} since:{since_date}"
            else:
                # M·∫∑c ƒë·ªãnh l·∫•y tweets trong 7 ng√†y qua
                since_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
                search_query = f"{query} since:{since_date}"
            
            print(f"Searching: {search_query}")
            
            # S·ª≠ d·ª•ng snscrape ƒë·ªÉ thu th·∫≠p tweets
            tweets = sntwitter.TwitterSearchScraper(search_query).get_items()
            
            count = 0
            for tweet in tweets:
                if count >= max_results:
                    break
                
                # Extract hashtags t·ª´ text
                hashtags = re.findall(r'#\w+', tweet.content)
                hashtags = [tag[1:] for tag in hashtags]  # B·ªè d·∫•u #
                
                tweet_doc = {
                    'tweet_id': str(tweet.id),
                    'text': tweet.content,
                    'created_at': tweet.date,
                    'likes': tweet.likeCount or 0,
                    'retweets': tweet.retweetCount or 0,
                    'replies': tweet.replyCount or 0,
                    'hashtags': hashtags,
                    'lang': tweet.lang or 'unknown',
                    'username': tweet.user.username,
                    'user_followers': tweet.user.followersCount or 0,
                    'topic': query,
                    'source': 'twitter',
                    'collected_at': datetime.now()
                }
                tweets_data.append(tweet_doc)
                count += 1
            
            return tweets_data
        
        except Exception as e:
            print(f"Error collecting tweets: {e}")
            return []
    
    def save_to_mongodb(self, tweets_data, mongo_uri):
        """L∆∞u v√†o MongoDB"""
        if tweets_data:
            client = MongoClient(mongo_uri)
            db = client['social_media_analysis']
            posts_collection = db['posts']
            
            # Ki·ªÉm tra tr√πng l·∫∑p tr∆∞·ªõc khi insert
            new_tweets = []
            for tweet in tweets_data:
                existing = posts_collection.find_one({'tweet_id': tweet['tweet_id']})
                if not existing:
                    new_tweets.append(tweet)
            
            if new_tweets:
                posts_collection.insert_many(new_tweets)
                print(f"‚úÖ Saved {len(new_tweets)} new tweets to MongoDB")
            else:
                print("‚ÑπÔ∏è No new tweets to save")

# S·ª≠ d·ª•ng
if __name__ == "__main__":
    # MongoDB URI
    MONGO_URI = "mongodb+srv://TrumBeoo:1xr1R8BRdLafRzTg@trumbeoo.c0hnfng.mongodb.net/social_media_analysis?" \
                "retryWrites=true&w=majority&appName=TrumBeoo"
    
    # Kh·ªüi t·∫°o crawler
    crawler = TwitterCrawler()
    
    # Thu th·∫≠p d·ªØ li·ªáu v·ªÅ c√°c ch·ªß ƒë·ªÅ
    topics = [
        "AI education",
        "tr√≠ tu·ªá nh√¢n t·∫°o gi√°o d·ª•c", 
        "AI h·ªçc t·∫≠p",
        "#AIEducation",
        "machine learning gi√°o d·ª•c"
    ]
    
    for topic in topics:
        print(f"üîç Collecting tweets about: {topic}")
        tweets = crawler.search_tweets(topic, max_results=50)
        crawler.save_to_mongodb(tweets, MONGO_URI)
        print(f"Found {len(tweets)} tweets\n")